{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces some of the basic tools and idea for working with natural language (text), namely tokenization. Tokenization is the process of turning text into a sequence of words, with punctuation and common (stop) words removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = '`~!@#$%^&*()_-+={[}]|\\:;\"<,>.?/}\\t\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `remove_punctuation`, that removes punctuation from an iterator of words and yields the cleaned words:\n",
    "\n",
    "* Strip the punctuation characters at the beginning and end of each word.\n",
    "* Replace the character `-` by a space if found in the middle of the word and split on that white space to yield multiple words.\n",
    "* If a word is all punctuation, don't yield it at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4dc4989ce093c26ce59fc407fee06d68",
     "grade": false,
     "grade_id": "tokenizationa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(words, punctuation=PUNCTUATION):\n",
    "    \"\"\"Remove punctuation from an iterator of words, yielding the results.\"\"\"\n",
    "    print(words)\n",
    "    arr = []\n",
    "    stripF = 0;\n",
    "    stripB = len(words);\n",
    "    talk = words[0][stripF:stripB]\n",
    "    print(talk)\n",
    "    print(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86485460b6297d3169a3951bf3d4a77f",
     "grade": true,
     "grade_id": "tokenizationb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!data;']\n",
      "!\n",
      "!data;\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-78edb0dc0caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'!data;'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'!data-science:'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'science'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'!!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'!!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "assert list(remove_punctuation(['!data;']))==['data']\n",
    "assert list(remove_punctuation(['!data-science:']))==['data', 'science']\n",
    "assert list(remove_punctuation(['!!']))==[]\n",
    "assert isinstance(remove_punctuation(['!!']), types.GeneratorType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `lower_words`, that makes each word in an iterator lowercase, yielding each lowercase word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f4eca9c5ba47afe1f6416acffb7193db",
     "grade": false,
     "grade_id": "tokenizationc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lower_words(words):\n",
    "    \"\"\"Make each word in an iterator lowercase.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "934dcedff3b6d14a88eebf72962d8d53",
     "grade": true,
     "grade_id": "tokenizationd",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(lower_words('AAA'), types.GeneratorType)\n",
    "assert list(lower_words('This IS NOT LoWerCaSe'.split(' ')))==['this', 'is', 'not', 'lowercase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stop words](https://en.wikipedia.org/wiki/Stop_words) are common words in text that are typically filtered out when performing natural language processing. Typical stop words are *and*, *of*, *a*, *the*, etc.\n",
    "\n",
    "Write a generator function, `remove_stop_words`, that removes stop words from an iterator, yielding the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "37bb9b25d0d76edac771593474db2448",
     "grade": false,
     "grade_id": "tokenizatione",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(words, stop_words=None):\n",
    "    \"\"\"Remove the stop words from an iterator of words.\n",
    "    \n",
    "    stop_words can be provided as a list of words or a whitespace separated string of words.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c56ec266c7b0ccb6afcb66568584682",
     "grade": true,
     "grade_id": "tokenizationf",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '), stop_words='a the')) == \\\n",
    "    ['begin', 'to', 'end', 'of', 'day']\n",
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '), stop_words=['a', 'the'])) == \\\n",
    "    ['begin', 'to', 'end', 'of', 'day']\n",
    "assert list(remove_stop_words('the begin to the end a of the day'.split(' '))) == \\\n",
    "    ['the', 'begin', 'to', 'the', 'end', 'a', 'of', 'the', 'day']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) is the process of taking a string or line of text and returning a sequence of words, or *tokens*, with the following transforms applied\n",
    "\n",
    "* Punctuation removed\n",
    "* All words lowercased\n",
    "* Stop words removed\n",
    "\n",
    "Write a generator function, `tokenize_line`, that yields tokenized words from an input line of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f11462d2bbc8e8504892362b48ed8426",
     "grade": false,
     "grade_id": "tokenizationg",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_line(line, stop_words=None, punctuation=PUNCTUATION):\n",
    "    \"\"\"Split a string into a list of words, removing punctuation and stop words.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f350ab4389b57fd807b2c339489a513",
     "grade": true,
     "grade_id": "tokenizationh",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(tokenize_line(\"This, is the way; that things will end\"), types.GeneratorType)\n",
    "assert list(tokenize_line(\"This, is the way; that things will end\", stop_words=['the', 'is'])) == \\\n",
    "    ['this', 'way', 'that', 'things', 'will', 'end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a generator function, `tokenize_lines`, that can yield the tokens in an iterator of lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4226ab291a98f8faaf87359d35ea6ddc",
     "grade": false,
     "grade_id": "tokenizationi",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_lines(lines, stop_words=None, punctuation=PUNCTUATION):\n",
    "    \"\"\"Tokenize an iterator of lines, yielding the tokens.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e788cdfd3130f7ac7eaebc44b6c2b36",
     "grade": true,
     "grade_id": "tokenizationj",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "wasteland = \"\"\"\n",
    "APRIL is the cruellest month, breeding\n",
    "Lilacs out of the dead land, mixing\n",
    "Memory and desire, stirring\n",
    "Dull roots with spring rain.\n",
    "\"\"\"\n",
    "\n",
    "assert isinstance(tokenize_lines(wasteland.splitlines()), types.GeneratorType)\n",
    "\n",
    "assert list(tokenize_lines(wasteland.splitlines(), stop_words='is the of and')) == \\\n",
    "    ['april','cruellest','month','breeding','lilacs','out','dead','land',\n",
    "     'mixing','memory','desire','stirring','dull','roots','with','spring',\n",
    "     'rain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize song lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use all of the above functions to perform tokenization on the set of song lyrics from this Kaggle hosted dataset:\n",
    "\n",
    "https://www.kaggle.com/mousehead/songlyrics\n",
    "\n",
    "* You should be able to perform this in a memory efficient manner.\n",
    "* Read your stop words from the included `data/stopwords.txt` file.\n",
    "\n",
    "Here is the dataset loaded as a Pandas `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c88995b7143dd2c229d8795cb9a96367",
     "grade": false,
     "grade_id": "tokenizationk",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/songlyrics/songdata.csv\")\n",
    "df.head()\n",
    "lyrics = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we extract the `text` column, we get an iterator over the lyrics. **Remember each lyric can and will have multiple lines.** Here is the total number of lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "62eb2ff33c62820b26d0493420f1cea9",
     "grade": false,
     "grade_id": "tokenizationl",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "len(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file `data/stopwords.txt` and tokenize the file into a list of stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "eeefdb32870a44e3a7753d3a53b1748f",
     "grade": false,
     "grade_id": "tokenizationm",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7c4b51149ebc21a1d13f0de5e1c86246",
     "grade": true,
     "grade_id": "tokenizationn",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(stop_words)==174\n",
    "assert type(stop_words)==list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b87ae8275dc818262c9c338e5da78c68",
     "grade": false,
     "grade_id": "tokenizationo",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate through the lyrics and for each lyric:\n",
    "\n",
    "1. Split the lyric into lines.\n",
    "2. Call `tokenize_lines` on the lyric eliminating the above stop words (and punctuation).\n",
    "3. Count the total number of words across all lyrics (excluding stop words) and save the result in a variable named `nwords`.\n",
    "\n",
    "If you do all of this in a memory efficient manner, the total memory consumption of this notebook shouldn't go over around 250MB. Most of that is just using Pandas to read the dataset into memory. If you construct a full list of all the words and *then* count them all, your memory consumption will be 3-4x that. This should only take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f52ab8e9c854e0fb9a727323cb917a2e",
     "grade": true,
     "grade_id": "tokenizationp",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04e1a25a3c7fff8baa8e89c7017f5c22",
     "grade": false,
     "grade_id": "tokenizationq",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total number of words: {}\".format(nwords))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
