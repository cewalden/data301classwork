{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a complex model with a small amount of complex data will overfit the data to the model\n",
    "\n",
    "for a given complexity model, there's only a dataset of a certain size that it can memorize\n",
    "\n",
    "going over that forces the model to extract the underlying trends\n",
    "\n",
    "the more data you have, the more complex models you will be able to build before overfitting and having score of new data go down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training size\n",
    ">^\n",
    ">|\n",
    ">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i would recommend dropping first in one-hot encoding \n",
    "\n",
    "sklearn.preprocessing.OneHotEncoder\n",
    "\n",
    "pd.get_dummies\n",
    "\n",
    "one thing that's nice about one hot encoding- if u have missing data, it can drop that column\n",
    "\n",
    "if u want to do raw word counts, have to remove the stop words yourself\n",
    "\n",
    "TFIDF (?) willl do that for you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
